# configs/indoor_cr64_enhanced.yaml - Enhanced 1/64 compression ratio configuration
model:
  name: CRLSTMNet
  spatial:
    res_blocks: 5                    # Increase number of residual blocks
    channels: 40                     # Increase channels (32->40)
    latent_dim_high: 320             # Increase high-dim latent (256->320)
    latent_dim_low: 80               # Increase low-dim latent (64->80)
    negative_slope: 0.3
    output_activation: linear
  temporal:
    layers: 3                        # Increase LSTM layers (2->3)
    bidirectional: false
  sequence:
    T: 10

training:
  batch_size: 16                     # Increase batch size (8->16)
  num_workers: 0
  amp: true                          # Enable mixed precision
  grad_clip: 1.0
  optimizer: adamw
  weight_decay: 1e-4                 # Increase regularization
  pin_memory: true
  persistent_workers: false

  # New: Gradient accumulation settings
  gradient_accumulation_steps: 2     # Effective batch size = 16*2 = 32

  # New: Enhanced learning rate scheduling
  use_onecycle: true                 # Use OneCycleLR
  max_lr_factor: 5.0                 # Max LR is 5x base lr

# Stage-specific settings (1/64 enhanced version)
stage0:
  epochs: 60                         # Increase epochs
  lr: 1e-3
  warmup_epochs: 10
  cosine: true
  validate_every: 5
  early_stop:
    patience: 15
    min_delta: 0.0003               # More sensitive stopping condition

  # New: Multi-scale training
  multi_scale: true
  scale_factors: [0.8, 1.0, 1.2]

stage1:
  epochs: 50                         # Significantly increase stage1 training
  lr: 5e-4
  validate_every: 3
  early_stop:
    patience: 12
    min_delta: 0.0003
  latent_consistency_weight: 0.15    # Increase consistency loss weight

  # New: Temporal enhancement settings
  temporal_regularization: 0.05      # Temporal regularization weight
  use_temporal_consistency: true     # Enable temporal consistency loss

stage2:
  epochs: 40                         # Increase stage2 training
  lr: 2e-4
  validate_every: 5
  early_stop:
    patience: 20                     # Increase patience
    min_delta: 0.0003

  # New: End-to-end fine-tuning enhancement
  lr_decay_factor: 0.5               # Learning rate decay factor
  lr_decay_patience: 8               # Learning rate decay patience
  use_cosine_restart: true           # Use cosine restart

loss:
  nmse: 1.0
  cos: 0.2                           # Significantly increase cosine similarity weight (0.05->0.2)
  tsmooth: 0.08                      # Increase temporal smoothness weight

  # New: Frequency domain loss
  freq_domain: 0.1                   # Frequency domain loss weight

  # New: Correlation coefficient loss
  correlation: 0.15                  # Direct correlation coefficient optimization

data:
  root: F:\CRLSTMNet\data\cost2100
  indoor_path: indoor_20slots
  pdiff_path: P_diff_T
  dataset_spec:
    prefix: H_user_t
    suffix: 32all.mat
    variable_name: Hur_down_t1
    train_split: 0.8
  split: indoor
  snr: 30
  cr: 1/64
  cr_rate: 0.015625
  cr_num: 16
  speed: 30
  normalize: standardize
  use_pdiff: false
  cache_data: true
  prefetch_factor: 2
  allow_dummy_data: false           # Strictly disable dummy data

logging:
  project: crlstm_cost2100_cr64_enhanced
  run_name: indoor_cr64_experiment_enhanced
  save_dir: ./checkpoints/indoor_cr64_enhanced
  log_interval: 10
  save_top_k: 3
  max_val_batches: 100

# Hardware optimization settings
device: cuda
seed: 2025
deterministic: false
benchmark: true

# Model naming configuration
model_naming:
  dataset_type: indoor
  compression_ratio: "1_64_enhanced"
  include_timestamp: true

# Complexity analysis configuration
analyze_complexity: true

# Training monitoring configuration
monitoring:
  use_tqdm: true
  save_metrics: true
  metrics_file: metrics_indoor_cr64_enhanced.json

  # New: Detailed monitoring
  save_learning_curves: true
  plot_frequency: 10

# New: Data augmentation settings
data_augmentation:
  enabled: true
  noise_std: 0.01                   # Add small amount of noise
  channel_shuffle: true             # Channel random shuffling
  temporal_shift: true              # Temporal random shift

# New: Model architecture enhancements
architecture_enhancements:
  use_attention: false              # Temporarily disable attention mechanism
  use_residual_connections: true    # Enable residual connections
  use_channel_attention: true       # Enable channel attention
  use_spatial_attention: false      # Disable spatial attention

# New: Advanced training techniques
advanced_training:
  use_ema: true                     # Exponential moving average
  ema_decay: 0.999
  use_label_smoothing: false        # Label smoothing
  use_mixup: false                  # Mixup data augmentation

  # Learning rate finder
  lr_finder: false                  # Whether to use learning rate search

  # Knowledge distillation settings
  use_distillation: true            # Enable knowledge distillation
  teacher_temperature: 4.0          # Teacher network temperature
  distillation_alpha: 0.3           # Distillation loss weight

# Resume training settings
resume:
  enabled: true                     # Enable resume training
  checkpoint_path: null
  load_optimizer: true
  load_scheduler: true
  load_best_metric: true